import streamlit as st
import os
import pandas as pd
from io import StringIO
import re
import torch
import torch.nn as nn
from torch.nn import CrossEntropyLoss
from transformers import RobertaForSequenceClassification, RobertaConfig, get_linear_schedule_with_warmup, set_seed
import numpy as np
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
from peft import PeftModel, PeftConfig
import random
from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset
from scipy.special import softmax

import string
import tree_sitter
from tree_sitter import Language, Parser
import codecs

from model import Model
from predict import predict
from data_preprocessing import obfuscate

#–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—É–ª –Ω–∞—à–∏—Ö —è–∑—ã–∫–æ–≤
Language.build_library(
  # Store the library in the `build` directory
  'build/my-languages.so',

  # Include one or more languages
  [
    'tree-sitter-c-sharp'
  ]
)

#–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞—Ä—Å–µ—Ä –¥–ª—è C#
parser = Parser()
CSHARP_LANGUAGE = Language('build/my-languages.so', 'c_sharp')
parser.set_language(CSHARP_LANGUAGE)



#–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥–∏–∫—Ç–∞
base = 'microsoft/unixcoder-base'  # 'microsoft/unixcoder-base' 'microsoft/codebert-base'
model_id = "saved_models"  # '/kaggle/input/linevul-adapter' '/kaggle/input/unixcoder-adapter'

tokenizer = AutoTokenizer.from_pretrained(base)
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
n_gpu = torch.cuda.device_count()
set_seed(n_gpu)

config = RobertaConfig.from_pretrained(base)
config.num_labels = 1
model = RobertaForSequenceClassification.from_pretrained(base, config=config, ignore_mismatched_sizes=True).to(device)
model = Model(model, config, tokenizer)
model.to(device)

config = PeftConfig.from_pretrained(model_id)
model = PeftModel.from_pretrained(model, model_id)

#–í—ã–≤–æ–¥ –Ω–∞ —ç–∫—Ä–∞–Ω
st.title('–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –≤ –∫–æ–¥–µ C#')

st.markdown('–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, **—É—è–∑–≤–∏–º** –ª–∏ –≤–∞—à –∫–æ–¥.')
st.markdown('–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤–∞—à —Ñ–∞–π–ª —Å –∫–æ–¥–æ–º :point_down:')

uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª")
if uploaded_file is not None:
    with st.spinner('–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∞—à–∏—Ö —Å–µ–∫—Ä–µ—Ç–∏–∫–æ–≤...'):
        # To convert to a string based IO:
        stringio = StringIO(uploaded_file.getvalue().decode("utf-8"))
        # To read file as string:
        string_data = stringio.read()
        string_data = obfuscate(parser,string_data)
        res = predict(model, tokenizer, string_data, device, do_linelevel_preds = True)
        df = pd.DataFrame(res)
    st.markdown('**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:**')
    if 1 in df['vulnerable'].unique():
        st.error('–ù–∞–π–¥–µ–Ω—ã —É—è–∑–≤–∏–º–æ—Å—Ç–∏', icon="üö®")
        st.dataframe(df[df['vulnerable'] == 1])
    else:
        st.success('–£—è–∑–≤–∏–º–æ—Å—Ç–µ–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ', icon="‚úÖ")
